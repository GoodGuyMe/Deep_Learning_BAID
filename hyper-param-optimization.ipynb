{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 7908673,
     "sourceType": "datasetVersion",
     "datasetId": 4642477
    },
    {
     "sourceId": 7908740,
     "sourceType": "datasetVersion",
     "datasetId": 4645951
    },
    {
     "sourceId": 19078,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 15809
    },
    {
     "sourceId": 19540,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 16205
    }
   ],
   "dockerImageVersionId": 30673,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg19, VGG19_Weights, resnet50, ResNet50_Weights, resnet18\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T14:49:15.465697Z",
     "iopub.execute_input": "2024-03-29T14:49:15.466085Z",
     "iopub.status.idle": "2024-03-29T14:49:15.473257Z",
     "shell.execute_reply.started": "2024-03-29T14:49:15.466050Z",
     "shell.execute_reply": "2024-03-29T14:49:15.472167Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mean = [0.485, 0.456, 0.406]  # RGB\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "class BBDataset(Dataset):\n",
    "    def __init__(self, file_dir='dataset', type='train', test=False):\n",
    "        self.if_test = test\n",
    "        self.train_transformer = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=mean, std=std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.test_transformer = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=mean, std=std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.images = []\n",
    "        self.pic_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        if type == 'train':\n",
    "            DATA = pd.read_csv(os.path.join(file_dir, 'train_set.csv'))\n",
    "        elif type == 'validation':\n",
    "            DATA = pd.read_csv(os.path.join(file_dir, 'val_set.csv'))\n",
    "        elif type == 'test':\n",
    "            DATA = pd.read_csv(os.path.join(file_dir, 'test_set.csv'))\n",
    "\n",
    "        labels = DATA['score'].values.tolist()\n",
    "        pic_paths = DATA['image'].values.tolist()\n",
    "        for i in tqdm(range(len(pic_paths))):\n",
    "            pic_path = os.path.join('/kaggle/input/cs4240-78-1', pic_paths[i])\n",
    "            label = float(labels[i] / 10)\n",
    "            self.pic_paths.append(pic_path)\n",
    "            self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pic_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pic_path = self.pic_paths[index]\n",
    "        img = cv.imread(pic_path)\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        if self.if_test:\n",
    "            img = self.test_transformer(img)\n",
    "        else:\n",
    "            img = self.train_transformer(img)\n",
    "\n",
    "        return img, self.labels[index]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T14:49:15.482290Z",
     "iopub.execute_input": "2024-03-29T14:49:15.482623Z",
     "iopub.status.idle": "2024-03-29T14:49:15.495687Z",
     "shell.execute_reply.started": "2024-03-29T14:49:15.482598Z",
     "shell.execute_reply": "2024-03-29T14:49:15.494897Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def calc_mean_std(features):\n",
    "    batch_size, c = features.size()[:2]\n",
    "    features_mean = features.reshape(batch_size, c, -1).mean(dim=2).reshape(batch_size, c, 1, 1)\n",
    "    features_std = features.reshape(batch_size, c, -1).std(dim=2).reshape(batch_size, c, 1, 1) + 1e-6\n",
    "    return features_mean, features_std\n",
    "\n",
    "\n",
    "def adain(content_features, style_features):\n",
    "    content_mean, content_std = calc_mean_std(content_features)\n",
    "    style_mean, style_std = calc_mean_std(style_features)\n",
    "    normalized_features = style_std * (content_features - content_mean) / content_std + style_mean\n",
    "    return normalized_features\n",
    "\n",
    "\n",
    "class NonLocalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
    "        super(NonLocalBlock, self).__init__()\n",
    "\n",
    "        self.sub_sample = sub_sample\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.inter_channels = inter_channels\n",
    "\n",
    "        if self.inter_channels is None:\n",
    "            self.inter_channels = in_channels // 2\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1\n",
    "\n",
    "        conv_nd = nn.Conv2d\n",
    "        max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        bn = nn.BatchNorm2d\n",
    "\n",
    "        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                         kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        if bn_layer:\n",
    "            self.W = nn.Sequential(\n",
    "                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                        kernel_size=1, stride=1, padding=0),\n",
    "                bn(self.in_channels)\n",
    "            )\n",
    "            nn.init.constant_(self.W[1].weight, 0)\n",
    "            nn.init.constant_(self.W[1].bias, 0)\n",
    "        else:\n",
    "            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "            nn.init.constant_(self.W.weight, 0)\n",
    "            nn.init.constant_(self.W.bias, 0)\n",
    "\n",
    "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                           kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        if sub_sample:\n",
    "            self.g = nn.Sequential(self.g, max_pool_layer)\n",
    "            self.phi = nn.Sequential(self.phi, max_pool_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
    "        g_x = g_x.permute(0, 2, 1)\n",
    "\n",
    "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
    "        theta_x = theta_x.permute(0, 2, 1)\n",
    "        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
    "        f = torch.matmul(theta_x, phi_x)\n",
    "        f_div_C = F.softmax(f, dim=-1)\n",
    "\n",
    "        y = torch.matmul(f_div_C, g_x)\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
    "        W_y = self.W(y)\n",
    "        z = W_y + x\n",
    "\n",
    "        return z"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T14:49:15.593579Z",
     "iopub.execute_input": "2024-03-29T14:49:15.593839Z",
     "iopub.status.idle": "2024-03-29T14:49:15.611422Z",
     "shell.execute_reply.started": "2024-03-29T14:49:15.593817Z",
     "shell.execute_reply": "2024-03-29T14:49:15.610508Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class VGG(nn.Module):   # output relu4-1\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        model = vgg19()\n",
    "        model.load_state_dict(torch.load('/kaggle/input/baid/pytorch/fully_pretrained/2/vgg19-dcbb9e9d.pth'),\n",
    "                                        strict=False)\n",
    "        self.model = nn.Sequential(*model.features[:21])\n",
    "        self._freeze_params()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def _freeze_params(self):\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, identity=False):\n",
    "        super(SAB, self).__init__()\n",
    "        model = resnet50()\n",
    "        self.model = nn.Module()\n",
    "        self.model.conv1 = model.conv1\n",
    "        self.model.bn1 = model.bn1\n",
    "        self.model.relu = model.relu\n",
    "        self.model.maxpool = model.maxpool\n",
    "\n",
    "        self.model.layer1 = model.layer1\n",
    "        self.model.layer2 = model.layer2\n",
    "\n",
    "        self.identity = identity\n",
    "\n",
    "        self.vgg = VGG()\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # aligned to the output of VGG\n",
    "        sty = self.vgg(x)\n",
    "        aes = self.model.conv1(x)\n",
    "        aes = self.model.bn1(aes)\n",
    "        aes = self.model.relu(aes)\n",
    "        aes = self.model.maxpool(aes)\n",
    "\n",
    "        aes = self.model.layer1(aes)\n",
    "        aes = self.model.layer2(aes)\n",
    "\n",
    "        output = adain(aes, sty)\n",
    "        if self.identity:\n",
    "            output += aes\n",
    "\n",
    "        return F.relu(output)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.load_state_dict(torch.load('/kaggle/input/baid/pytorch/fully_pretrained/2/epoch_99.pth'),\n",
    "                                        strict=False)\n",
    "        else:\n",
    "            self.model.load_state_dict(torch.load('/kaggle/input/baid/pytorch/fully_pretrained/2/epoch_99.pth', map_location=torch.device('cpu')),\n",
    "                                        strict=False)\n",
    "\n",
    "\n",
    "class GAB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAB, self).__init__()\n",
    "        model = resnet50()\n",
    "        self.model = nn.Module()\n",
    "        self.model.conv1 = model.conv1\n",
    "        self.model.bn1 = model.bn1\n",
    "        self.model.relu = model.relu\n",
    "        self.model.maxpool = model.maxpool\n",
    "\n",
    "        self.model.layer1 = model.layer1\n",
    "        self.model.layer2 = model.layer2\n",
    "        self.model.layer3 = model.layer3\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _init_weights(self):\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.load_state_dict(torch.load('/kaggle/input/baid/pytorch/fully_pretrained/2/epoch_99.pth'),\n",
    "                                        strict=False)\n",
    "        else:\n",
    "            self.model.load_state_dict(torch.load('/kaggle/input/baid/pytorch/fully_pretrained/2/epoch_99.pth', map_location=torch.device('cpu')),\n",
    "                                        strict=False)\n",
    "class SAAN(nn.Module):\n",
    "    def __init__(self, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.GenAes = GAB()\n",
    "        self.StyAes = SAB()\n",
    "\n",
    "        self.NLB = NonLocalBlock(in_channels=1536)\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=(2, 2))\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(num_features=1536)\n",
    "\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(1536 * 4, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(2048, num_classes),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self._initial_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gen_aes = self.GenAes(x)\n",
    "        sty_aes = self.StyAes(x)\n",
    "\n",
    "        sty_aes = self.max_pool(sty_aes)\n",
    "\n",
    "        all_aes = torch.cat((sty_aes, gen_aes), 1)\n",
    "        all_aes = self.NLB(all_aes)\n",
    "\n",
    "        all_aes = self.avg_pool(all_aes)\n",
    "        all_aes = self.bn(all_aes)\n",
    "\n",
    "        fc_input = torch.flatten(all_aes, start_dim=1)\n",
    "\n",
    "        output = self.predictor(fc_input)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _initial_weights(self):\n",
    "        for m in self.bn.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.normal_(m.weight.data, mean=1.0, std=0.02)\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "        for m in self.predictor.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=0.02)\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "class ResNetPretrain(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetPretrain, self).__init__()\n",
    "        model = resnet50()\n",
    "        self.model = model\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=2048, out_features=num_classes, bias=True),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        self._initial_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        features = self.model.layer4(x)\n",
    "        features_flat = self.model.avgpool(features)\n",
    "        features_flat = torch.flatten(features_flat, 1)\n",
    "        output = self.model.fc(features_flat)\n",
    "\n",
    "        return features, output\n",
    "\n",
    "    def _initial_weights(self):\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "                \n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-03-29T14:49:15.625094Z",
     "iopub.execute_input": "2024-03-29T14:49:15.625362Z",
     "iopub.status.idle": "2024-03-29T14:49:15.657937Z",
     "shell.execute_reply.started": "2024-03-29T14:49:15.625340Z",
     "shell.execute_reply": "2024-03-29T14:49:15.657046Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "n_epochs = 100\n",
    "batch_size = 64\n",
    "validation_frequency = 2\n",
    "save_frequency = 2\n",
    "learning_rate = 1e-5\n",
    "device = \"cuda\"\n",
    "checkpoint_dir = \"/kaggle/working/SAAN\"\n",
    "\n",
    "train_dataset = BBDataset(file_dir='/kaggle/input/baid-csvs', type='train', test=False)\n",
    "val_dataset = BBDataset(file_dir='/kaggle/input/baid-csvs', type='validation', test=True)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(learning_rate, optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 10 epochs\"\"\"\n",
    "    if epoch < 40:\n",
    "        lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "    else:\n",
    "        lr = learning_rate * (0.1 ** 4)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def save_checkpoint(checkpoint_dir, model, epoch):\n",
    "    checkpoint_dir = checkpoint_dir\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    save_path = os.path.join(checkpoint_dir, f\"epoch_{epoch}.pth\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "def validate(model, val_loader, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for step, val_data in enumerate(val_loader):\n",
    "            image = val_data[0].to(device)\n",
    "            label = val_data[1].to(device).float()\n",
    "\n",
    "            predicted_label = model(image).squeeze()\n",
    "            val_loss += loss(predicted_label, label).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(\"Epoch: %3d Validation loss: %.8f\" % (epoch, val_loss))\n",
    "    return val_loss\n",
    "\n",
    "def train():\n",
    "\n",
    "    model = SAAN(num_classes=1)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'GenAes' in name:\n",
    "            param.requires_grad = False\n",
    "    model = model.to(device)\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999), weight_decay=5e-4)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=8)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for step, train_data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            image = train_data[0].to(device)\n",
    "            label = train_data[1].to(device).float()\n",
    "\n",
    "            predicted_label = model(image).squeeze()\n",
    "            train_loss = loss(predicted_label, label)\n",
    "\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += train_loss.item()\n",
    "\n",
    "            print(\"Epoch: %3d Step: %5d / %5d Train loss: %.8f\" % (epoch, step, len(train_loader), train_loss.item()))\n",
    "\n",
    "        adjust_learning_rate(learning_rate, optimizer, epoch)\n",
    "\n",
    "        if (epoch + 1) % validation_frequency == 0:\n",
    "            validate(model, val_loader, epoch)\n",
    "            print(\"Validation\")\n",
    "\n",
    "        if (epoch + 1) % save_frequency == 0:\n",
    "            save_checkpoint(checkpoint_dir, model, epoch)\n",
    "            print(\"Saved\")\n",
    "        \n",
    "\n",
    "    print(\"Done training\")\n",
    "\n",
    "\n",
    "\n",
    "# Uncomment when you want to train\n",
    "# Might take up to a day to train on two GPU T4.\n",
    "#train()\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T14:49:15.659687Z",
     "iopub.execute_input": "2024-03-29T14:49:15.659978Z",
     "iopub.status.idle": "2024-03-29T14:49:15.912327Z",
     "shell.execute_reply.started": "2024-03-29T14:49:15.659955Z",
     "shell.execute_reply": "2024-03-29T14:49:15.911442Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_dataset = BBDataset(file_dir='/kaggle/input/baid-csvs', type='test', test=True)\n",
    "\n",
    "def test(model):\n",
    "    device = \"cuda\"\n",
    "    df = pd.read_csv('/kaggle/input/baid-csvs/test_set.csv')\n",
    "    predictions = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True, num_workers=4)\n",
    "    print(f\"Testing model:\")\n",
    "    with torch.no_grad():\n",
    "        for step, test_data in tqdm(enumerate(test_loader)):\n",
    "            image = test_data[0].to(device)\n",
    "\n",
    "            predicted_label = model(image)\n",
    "            prediction = predicted_label.squeeze().cpu().numpy()\n",
    "            predictions.append(prediction * 10)\n",
    "\n",
    "    scores = df['score'].values.tolist()\n",
    "\n",
    "    print(scipy.stats.spearmanr(scores, predictions))\n",
    "    print(scipy.stats.pearsonr(scores, predictions))\n",
    "\n",
    "    acc = 0\n",
    "    for i in range(len(scores)):\n",
    "        cls1 = 1 if scores[i] > 5 else 0\n",
    "        cls2 = 1 if predictions[i] > 5 else 0\n",
    "        if cls1 == cls2:\n",
    "            acc += 1\n",
    "    print(acc/len(scores))\n",
    "    df.insert(loc=2, column='prediction', value=predictions)\n",
    "    \n",
    "    save_dir = '/kaggle/working'\n",
    "\n",
    "    save_path = os.path.join(save_dir, 'result.csv')\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(\"Testing done\")\n",
    "    return acc/len(scores)\n",
    "\n",
    "# test()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T14:49:15.913909Z",
     "iopub.execute_input": "2024-03-29T14:49:15.914192Z",
     "iopub.status.idle": "2024-03-29T14:49:15.960687Z",
     "shell.execute_reply.started": "2024-03-29T14:49:15.914167Z",
     "shell.execute_reply": "2024-03-29T14:49:15.959710Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# n_epochs = 100\n",
    "# batch_size = 64\n",
    "# validation_frequency = 2\n",
    "# save_frequency = 2\n",
    "# learning_rate = 1e-5\n",
    "# device = \"cuda\"\n",
    "# checkpoint_dir = \"/kaggle/working/SAAN\"\n",
    "\n",
    "\n",
    "# num = 16\n",
    "# while num >= 1:\n",
    "#     for i in range(0, num):\n",
    "#         score[f\"id:{i}\"] = i\n",
    "#     sorted_items = sorted(score.items(), key=lambda x: x[1], reverse=True)\n",
    "#     print(sorted_items[:int(num/2)])\n",
    "#     num = int(num/2)\n",
    "\n",
    "def find_lowest(dictionary, n):\n",
    "    # Sort the dictionary by values\n",
    "    sorted_dict = sorted(dictionary.items(), key=lambda x: x[1])\n",
    "    \n",
    "    # Get the keys of the lowest n values\n",
    "    keys_to_delete = [item[0] for item in sorted_dict[:n]]\n",
    "    \n",
    "    # Delete the keys from the dictionary\n",
    "    for key in keys_to_delete:\n",
    "        del dictionary[key]\n",
    "        \n",
    "    return keys_to_delete\n",
    "        \n",
    "def gen_model(learning_rate, batch_size):\n",
    "    model = SAAN(num_classes=1)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'GenAes' in name:\n",
    "            param.requires_grad = False\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999), weight_decay=5e-4)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "    return (model, optimizer, train_loader, val_loader)\n",
    "\n",
    "    \n",
    "def successive_halving_train():\n",
    "    learning_rates = [1e-3, 1e-4, 1e-5, 1e-6]\n",
    "    models = {}\n",
    "    score = {}\n",
    "    num_models = 16\n",
    "    for i in range(num_models):\n",
    "        score[f'Model_{i}'] = 0\n",
    "        if i < num_models/4:\n",
    "            models[f'Model_{i}'] = gen_model(learning_rates[i%4], 64)\n",
    "        elif i < num_models/2:\n",
    "            models[f'Model_{i}'] = gen_model(learning_rates[i%4], 32)\n",
    "        elif i < 3*num_models/4:\n",
    "            models[f'Model_{i}'] = gen_model(learning_rates[i%4], 16)\n",
    "        else:\n",
    "            models[f'Model_{i}'] = gen_model(learning_rates[i%4], 8)\n",
    "    eta = 2\n",
    "    n_epochs_sh = 1\n",
    "    n_epoch = 0\n",
    "    num_models_2 = num_models\n",
    "    while num_models_2 >= 1:      \n",
    "        print(f\"Training models: {models.keys()} with {n_epochs_sh} epochs.\")\n",
    "        for model_key in models:\n",
    "            print(f\"Training model: {model_key}\")\n",
    "            model, optimizer, train_loader, val_loader = models[model_key]\n",
    "            model_state = train_2(n_epochs_sh, model, optimizer, train_loader, val_loader, n_epoch, model_key)\n",
    "            score[model_key] = test(model) # Is val loss good enough measure or should i use test()\n",
    "            models[model_key] = model_state, optimizer, train_loader, val_loader\n",
    "        num_models_2 = int(num_models_2/eta)\n",
    "        n_epoch = n_epoch + n_epochs_sh\n",
    "        n_epochs_sh = n_epochs_sh*eta\n",
    "        print(f\"Scores from models: {score}\")\n",
    "        keys_to_delete = find_lowest(score, num_models_2)\n",
    "        print(f\"Deleting models: {keys_to_delete}\")\n",
    "        for key in keys_to_delete:\n",
    "            del models[key]\n",
    "    \n",
    "    \n",
    "def save_checkpoint_2(checkpoint_dir, model, epoch, model_id):\n",
    "    checkpoint_dir = checkpoint_dir\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    save_path = os.path.join(checkpoint_dir, f\"epoch_{epoch}_hyper_param_{model_id}.pth\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    \n",
    "def train_2(n_epochs_sh, model, optimizer, train_loader, val_loader, n_epoch, model_id):\n",
    "    loss = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(n_epochs_sh):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for step, train_data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            image = train_data[0].to(device)\n",
    "            label = train_data[1].to(device).float()\n",
    "\n",
    "            predicted_label = model(image).squeeze()\n",
    "            train_loss = loss(predicted_label, label)\n",
    "\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += train_loss.item()\n",
    "\n",
    "            print(\"Epoch: %3d Step: %5d / %5d Train loss: %.8f\" % (epoch + n_epoch, step, len(train_loader), train_loss.item()))\n",
    "\n",
    "        adjust_learning_rate(learning_rate, optimizer, epoch)\n",
    "\n",
    "        if (epoch + 1) % validation_frequency == 0:\n",
    "            val_loss = validate(model, val_loader, epoch + n_epoch)\n",
    "            print(\"Validation\")\n",
    "\n",
    "        if (epoch + 1) % save_frequency == 0:\n",
    "            save_checkpoint_2(checkpoint_dir, model, epoch + n_epoch, model_id)\n",
    "            print(\"Saved\")\n",
    "        \n",
    "#     save_checkpoint_2(checkpoint_dir, model, epoch, modeel_id)\n",
    "    validate(model, val_loader, epoch)\n",
    "    return model\n",
    "    \n",
    "successive_halving_train()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T14:49:15.961776Z",
     "iopub.execute_input": "2024-03-29T14:49:15.962040Z",
     "iopub.status.idle": "2024-03-29T15:06:24.739491Z",
     "shell.execute_reply.started": "2024-03-29T14:49:15.962017Z",
     "shell.execute_reply": "2024-03-29T15:06:24.738177Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
