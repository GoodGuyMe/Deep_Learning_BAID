{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d2a6e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T12:29:18.995111Z",
     "iopub.status.busy": "2024-04-11T12:29:18.994776Z",
     "iopub.status.idle": "2024-04-11T12:29:27.539554Z",
     "shell.execute_reply": "2024-04-11T12:29:27.538758Z"
    },
    "papermill": {
     "duration": 8.552565,
     "end_time": "2024-04-11T12:29:27.541731",
     "exception": false,
     "start_time": "2024-04-11T12:29:18.989166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg19, VGG19_Weights, resnet50, ResNet50_Weights, resnet18\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92627b60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T12:29:27.550625Z",
     "iopub.status.busy": "2024-04-11T12:29:27.550032Z",
     "iopub.status.idle": "2024-04-11T12:29:27.563156Z",
     "shell.execute_reply": "2024-04-11T12:29:27.562351Z"
    },
    "papermill": {
     "duration": 0.019291,
     "end_time": "2024-04-11T12:29:27.564937",
     "exception": false,
     "start_time": "2024-04-11T12:29:27.545646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]  # RGB\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "class BBDataset(Dataset):\n",
    "    def __init__(self, file_dir='dataset', type='train', test=False):\n",
    "        self.if_test = test\n",
    "        self.train_transformer = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=mean, std=std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.test_transformer = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=mean, std=std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.images = []\n",
    "        self.pic_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        if type == 'train':\n",
    "            DATA = pd.read_csv(os.path.join(file_dir, 'train_set.csv'))\n",
    "        elif type == 'validation':\n",
    "            DATA = pd.read_csv(os.path.join(file_dir, 'val_set.csv'))\n",
    "        elif type == 'test':\n",
    "            DATA = pd.read_csv(os.path.join(file_dir, 'test_set.csv'))\n",
    "\n",
    "        labels = DATA['score'].values.tolist()\n",
    "        pic_paths = DATA['image'].values.tolist()\n",
    "        for i in tqdm(range(len(pic_paths))):\n",
    "            pic_path = os.path.join('/kaggle/input/cs4240-78-1', pic_paths[i])\n",
    "            label = float(labels[i] / 10)\n",
    "            self.pic_paths.append(pic_path)\n",
    "            self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pic_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pic_path = self.pic_paths[index]\n",
    "        img = cv.imread(pic_path)\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        if self.if_test:\n",
    "            img = self.test_transformer(img)\n",
    "        else:\n",
    "            img = self.train_transformer(img)\n",
    "\n",
    "        return img, self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03a63c19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T12:29:27.572894Z",
     "iopub.status.busy": "2024-04-11T12:29:27.572630Z",
     "iopub.status.idle": "2024-04-11T12:29:27.591800Z",
     "shell.execute_reply": "2024-04-11T12:29:27.590980Z"
    },
    "papermill": {
     "duration": 0.025386,
     "end_time": "2024-04-11T12:29:27.593752",
     "exception": false,
     "start_time": "2024-04-11T12:29:27.568366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_mean_std(features):\n",
    "    batch_size, c = features.size()[:2]\n",
    "    features_mean = features.reshape(batch_size, c, -1).mean(dim=2).reshape(batch_size, c, 1, 1)\n",
    "    features_std = features.reshape(batch_size, c, -1).std(dim=2).reshape(batch_size, c, 1, 1) + 1e-6\n",
    "    return features_mean, features_std\n",
    "\n",
    "\n",
    "def adain(content_features, style_features):\n",
    "    content_mean, content_std = calc_mean_std(content_features)\n",
    "    style_mean, style_std = calc_mean_std(style_features)\n",
    "    normalized_features = style_std * (content_features - content_mean) / content_std + style_mean\n",
    "    return normalized_features\n",
    "\n",
    "\n",
    "class NonLocalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
    "        super(NonLocalBlock, self).__init__()\n",
    "\n",
    "        self.sub_sample = sub_sample\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.inter_channels = inter_channels\n",
    "\n",
    "        if self.inter_channels is None:\n",
    "            self.inter_channels = in_channels // 2\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1\n",
    "\n",
    "        conv_nd = nn.Conv2d\n",
    "        max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        bn = nn.BatchNorm2d\n",
    "\n",
    "        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                         kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        if bn_layer:\n",
    "            self.W = nn.Sequential(\n",
    "                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                        kernel_size=1, stride=1, padding=0),\n",
    "                bn(self.in_channels)\n",
    "            )\n",
    "            nn.init.constant_(self.W[1].weight, 0)\n",
    "            nn.init.constant_(self.W[1].bias, 0)\n",
    "        else:\n",
    "            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "            nn.init.constant_(self.W.weight, 0)\n",
    "            nn.init.constant_(self.W.bias, 0)\n",
    "\n",
    "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                           kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        if sub_sample:\n",
    "            self.g = nn.Sequential(self.g, max_pool_layer)\n",
    "            self.phi = nn.Sequential(self.phi, max_pool_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
    "        g_x = g_x.permute(0, 2, 1)\n",
    "\n",
    "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
    "        theta_x = theta_x.permute(0, 2, 1)\n",
    "        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
    "        f = torch.matmul(theta_x, phi_x)\n",
    "        f_div_C = F.softmax(f, dim=-1)\n",
    "\n",
    "        y = torch.matmul(f_div_C, g_x)\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
    "        W_y = self.W(y)\n",
    "        z = W_y + x\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c8aa02",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-11T12:29:27.601932Z",
     "iopub.status.busy": "2024-04-11T12:29:27.601687Z",
     "iopub.status.idle": "2024-04-11T12:29:27.633950Z",
     "shell.execute_reply": "2024-04-11T12:29:27.633266Z"
    },
    "papermill": {
     "duration": 0.038456,
     "end_time": "2024-04-11T12:29:27.635694",
     "exception": false,
     "start_time": "2024-04-11T12:29:27.597238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VGG(nn.Module):   # output relu4-1\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        model = vgg19()\n",
    "        model.load_state_dict(torch.load('/kaggle/input/baid/pytorch/fully_pretrained/2/vgg19-dcbb9e9d.pth'),\n",
    "                                        strict=False)\n",
    "        self.model = nn.Sequential(*model.features[:21])\n",
    "        self._freeze_params()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def _freeze_params(self):\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, identity=False):\n",
    "        super(SAB, self).__init__()\n",
    "        model = resnet50()\n",
    "        self.model = nn.Module()\n",
    "        self.model.conv1 = model.conv1\n",
    "        self.model.bn1 = model.bn1\n",
    "        self.model.relu = model.relu\n",
    "        self.model.maxpool = model.maxpool\n",
    "\n",
    "        self.model.layer1 = model.layer1\n",
    "        self.model.layer2 = model.layer2\n",
    "\n",
    "        self.identity = identity\n",
    "\n",
    "        self.vgg = VGG()\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # aligned to the output of VGG\n",
    "        sty = self.vgg(x)\n",
    "        aes = self.model.conv1(x)\n",
    "        aes = self.model.bn1(aes)\n",
    "        aes = self.model.relu(aes)\n",
    "        aes = self.model.maxpool(aes)\n",
    "\n",
    "        aes = self.model.layer1(aes)\n",
    "        aes = self.model.layer2(aes)\n",
    "\n",
    "        output = adain(aes, sty)\n",
    "        if self.identity:\n",
    "            output += aes\n",
    "\n",
    "        return F.relu(output)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.load_state_dict(torch.load('/kaggle/input/baid/pytorch/fully_pretrained/2/epoch_99.pth', map_location='cuda:0'),\n",
    "                                        strict=False)\n",
    "        else:\n",
    "            self.model.load_state_dict(torch.load('/kaggle/input/baid/pytorch/fully_pretrained/2/epoch_99.pth', map_location=torch.device('cpu')),\n",
    "                                        strict=False)\n",
    "\n",
    "\n",
    "class GAB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAB, self).__init__()\n",
    "        model = resnet50()\n",
    "        self.model = nn.Module()\n",
    "        self.model.conv1 = model.conv1\n",
    "        self.model.bn1 = model.bn1\n",
    "        self.model.relu = model.relu\n",
    "        self.model.maxpool = model.maxpool\n",
    "\n",
    "        self.model.layer1 = model.layer1\n",
    "        self.model.layer2 = model.layer2\n",
    "        self.model.layer3 = model.layer3\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _init_weights(self):\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.load_state_dict(torch.load('/kaggle/input/baid/pytorch/fully_pretrained/2/epoch_99.pth', map_location='cuda:0'),\n",
    "                                        strict=False)\n",
    "        else:\n",
    "            self.model.load_state_dict(torch.load('/kaggle/input/baid/pytorch/fully_pretrained/2/epoch_99.pth', map_location=torch.device('cpu')),\n",
    "                                        strict=False)\n",
    "class SAAN(nn.Module):\n",
    "    def __init__(self, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.GenAes = GAB()\n",
    "        self.StyAes = SAB()\n",
    "\n",
    "        self.NLB = NonLocalBlock(in_channels=1536)\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=(2, 2))\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(num_features=1536)\n",
    "\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(1536 * 4, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(2048, num_classes),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self._initial_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gen_aes = self.GenAes(x)\n",
    "        sty_aes = self.StyAes(x)\n",
    "\n",
    "        sty_aes = self.max_pool(sty_aes)\n",
    "\n",
    "        all_aes = torch.cat((sty_aes, gen_aes), 1)\n",
    "        all_aes = self.NLB(all_aes)\n",
    "\n",
    "        all_aes = self.avg_pool(all_aes)\n",
    "        all_aes = self.bn(all_aes)\n",
    "\n",
    "        fc_input = torch.flatten(all_aes, start_dim=1)\n",
    "\n",
    "        output = self.predictor(fc_input)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _initial_weights(self):\n",
    "        for m in self.bn.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.normal_(m.weight.data, mean=1.0, std=0.02)\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "        for m in self.predictor.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=0.02)\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "class ResNetPretrain(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetPretrain, self).__init__()\n",
    "        model = resnet50()\n",
    "        self.model = model\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=2048, out_features=num_classes, bias=True),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        self._initial_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        features = self.model.layer4(x)\n",
    "        features_flat = self.model.avgpool(features)\n",
    "        features_flat = torch.flatten(features_flat, 1)\n",
    "        output = self.model.fc(features_flat)\n",
    "\n",
    "        return features, output\n",
    "\n",
    "    def _initial_weights(self):\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f99396b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T12:29:27.643627Z",
     "iopub.status.busy": "2024-04-11T12:29:27.643372Z",
     "iopub.status.idle": "2024-04-11T12:29:40.391347Z",
     "shell.execute_reply": "2024-04-11T12:29:40.390088Z"
    },
    "papermill": {
     "duration": 12.754742,
     "end_time": "2024-04-11T12:29:40.393743",
     "exception": false,
     "start_time": "2024-04-11T12:29:27.639001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50737/50737 [00:00<00:00, 327912.06it/s]\n",
      "100%|██████████| 3200/3200 [00:00<00:00, 282760.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = 0\n",
    "batch_size = 64\n",
    "validation_frequency = 2\n",
    "save_frequency = 2\n",
    "learning_rate = 1e-5\n",
    "device = \"cuda\"\n",
    "checkpoint_dir = \"/kaggle/working/SAAN\"\n",
    "\n",
    "train_dataset = BBDataset(file_dir='/kaggle/input/baid-csvs', type='train', test=False)\n",
    "val_dataset = BBDataset(file_dir='/kaggle/input/baid-csvs', type='validation', test=True)\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "def plot_loss(train_loss, val_loss, num_epochs):\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    \n",
    "    plt.plot(epochs, train_loss, label='Mean Training Loss')\n",
    "    plt.plot(epochs, val_loss, label='Mean Validation Loss')\n",
    "    \n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def adjust_learning_rate(learning_rate, optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 10 epochs\"\"\"\n",
    "    if epoch < 40:\n",
    "        lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "    else:\n",
    "        lr = learning_rate * (0.1 ** 4)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def save_checkpoint(checkpoint_dir, model, epoch, optimizer, loss):\n",
    "    checkpoint_dir = checkpoint_dir\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    save_path = os.path.join(checkpoint_dir, f\"epoch_{epoch}.pth\")\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, save_path)\n",
    "    \n",
    "def validate(model, val_loader, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for step, val_data in enumerate(val_loader):\n",
    "            image = val_data[0].to(device)\n",
    "            label = val_data[1].to(device).float()\n",
    "\n",
    "            predicted_label = model(image).squeeze()\n",
    "            val_loss += loss(predicted_label, label).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(\"Epoch: %3d Validation loss: %.8f\" % (epoch, val_loss))\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def train(use_checkpoint=None):\n",
    "\n",
    "    model = SAAN(num_classes=1)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'GenAes' in name:\n",
    "            param.requires_grad = False\n",
    "    model = model.to(device)\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999), weight_decay=5e-4)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=8)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "    save_checkpoint(checkpoint_dir, model, 0, optimizer, 0)\n",
    "    start_epoch = 0\n",
    "    if use_checkpoint is not None:\n",
    "        checkpoint = torch.load(use_checkpoint)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for step, train_data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            image = train_data[0].to(device)\n",
    "            label = train_data[1].to(device).float()\n",
    "\n",
    "            predicted_label = model(image).squeeze()\n",
    "            train_loss = loss(predicted_label, label)\n",
    "\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += train_loss.item()\n",
    "\n",
    "            print(\"Epoch: %3d Step: %5d / %5d Train loss: %.8f\" % (epoch, step, len(train_loader), train_loss.item()))\n",
    "\n",
    "        adjust_learning_rate(learning_rate, optimizer, epoch)\n",
    "\n",
    "        validation_loss =  validate(model, val_loader, epoch)\n",
    "        training_losses.append(epoch_loss/len(train_loader))\n",
    "        validation_losses.append(validation_loss)\n",
    "\n",
    "        save_checkpoint(checkpoint_dir, model, epoch, optimizer, epoch_loss)\n",
    "\n",
    "    print(\"Done training\")\n",
    "    return training_losses, validation_losses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Uncomment when you want to train\n",
    "# Might take up to a day to train on two GPU T4.\n",
    "loss_train, loss_val = train()\n",
    "#plot_loss(loss_train, loss_val, n_epochs)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ecee43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T12:29:40.403782Z",
     "iopub.status.busy": "2024-04-11T12:29:40.403503Z",
     "iopub.status.idle": "2024-04-11T12:29:40.449640Z",
     "shell.execute_reply": "2024-04-11T12:29:40.448671Z"
    },
    "papermill": {
     "duration": 0.053196,
     "end_time": "2024-04-11T12:29:40.451553",
     "exception": false,
     "start_time": "2024-04-11T12:29:40.398357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6400/6400 [00:00<00:00, 312094.33it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = BBDataset(file_dir='/kaggle/input/baid-csvs', type='test', test=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    device = \"cuda\"\n",
    "    checkpoint_path = \"/kaggle/input/baid/pytorch/trained_til_20/1/epoch_19.pth\"\n",
    "    df = pd.read_csv('/kaggle/input/baid-csvs/test_set.csv')\n",
    "    predictions = []\n",
    "\n",
    "    model = SAAN(num_classes=1)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(checkpoint_path)[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True, num_workers=8)\n",
    "    print(\"test\")\n",
    "    with torch.no_grad():\n",
    "        for step, test_data in tqdm(enumerate(test_loader)):\n",
    "            image = test_data[0].to(device)\n",
    "\n",
    "            predicted_label = model(image)\n",
    "            prediction = predicted_label.squeeze().cpu().numpy()\n",
    "            predictions.append(prediction * 10)\n",
    "\n",
    "    scores = df['score'].values.tolist()\n",
    "\n",
    "    print(scipy.stats.spearmanr(scores, predictions))\n",
    "    print(scipy.stats.pearsonr(scores, predictions))\n",
    "\n",
    "    acc = 0\n",
    "    for i in range(len(scores)):\n",
    "        cls1 = 1 if scores[i] > 5 else 0\n",
    "        cls2 = 1 if predictions[i] > 5 else 0\n",
    "        if cls1 == cls2:\n",
    "            acc += 1\n",
    "    print(acc/len(scores))\n",
    "    df.insert(loc=2, column='prediction', value=predictions)\n",
    "    \n",
    "    save_dir = '/kaggle/working'\n",
    "\n",
    "    save_path = os.path.join(save_dir, 'result.csv')\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(\"done\")\n",
    "\n",
    "#test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6267c1",
   "metadata": {
    "papermill": {
     "duration": 0.004111,
     "end_time": "2024-04-11T12:29:40.460050",
     "exception": false,
     "start_time": "2024-04-11T12:29:40.455939",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4642477,
     "sourceId": 7908673,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4645951,
     "sourceId": 7908740,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 15809,
     "sourceId": 19078,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 16205,
     "sourceId": 19540,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 17919,
     "sourceId": 21633,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 18046,
     "sourceId": 21797,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 16205,
     "sourceId": 22019,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 24460,
     "sourceId": 29042,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25.890297,
   "end_time": "2024-04-11T12:29:42.085841",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-11T12:29:16.195544",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
