{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2082df83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T08:18:31.540429Z",
     "iopub.status.busy": "2024-04-05T08:18:31.540050Z",
     "iopub.status.idle": "2024-04-05T08:18:41.216806Z",
     "shell.execute_reply": "2024-04-05T08:18:41.215682Z"
    },
    "papermill": {
     "duration": 9.684981,
     "end_time": "2024-04-05T08:18:41.219557",
     "exception": false,
     "start_time": "2024-04-05T08:18:31.534576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg19, VGG19_Weights, resnet50, ResNet50_Weights, resnet18\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "879999f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T08:18:41.229876Z",
     "iopub.status.busy": "2024-04-05T08:18:41.228804Z",
     "iopub.status.idle": "2024-04-05T08:18:41.243029Z",
     "shell.execute_reply": "2024-04-05T08:18:41.242315Z"
    },
    "papermill": {
     "duration": 0.020923,
     "end_time": "2024-04-05T08:18:41.244962",
     "exception": false,
     "start_time": "2024-04-05T08:18:41.224039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]  # RGB\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "class BBDataset(Dataset):\n",
    "    def __init__(self, file_dir='dataset', type='train', test=False):\n",
    "        self.if_test = test\n",
    "        self.train_transformer = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=mean, std=std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.test_transformer = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=mean, std=std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.images = []\n",
    "        self.pic_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        if type == 'train':\n",
    "            DATA = pd.read_csv(os.path.join(file_dir, 'train_set.csv'))\n",
    "        elif type == 'validation':\n",
    "            DATA = pd.read_csv(os.path.join(file_dir, 'val_set.csv'))\n",
    "        elif type == 'test':\n",
    "            DATA = pd.read_csv(os.path.join(file_dir, 'test_set.csv'))\n",
    "\n",
    "        labels = DATA['score'].values.tolist()\n",
    "        pic_paths = DATA['image'].values.tolist()\n",
    "        for i in tqdm(range(len(pic_paths))):\n",
    "            pic_path = os.path.join('/kaggle/input/cs4240-78-1', pic_paths[i])\n",
    "            label = float(labels[i] / 10)\n",
    "            self.pic_paths.append(pic_path)\n",
    "            self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pic_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pic_path = self.pic_paths[index]\n",
    "        img = cv.imread(pic_path)\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        if self.if_test:\n",
    "            img = self.test_transformer(img)\n",
    "        else:\n",
    "            img = self.train_transformer(img)\n",
    "\n",
    "        return img, self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e2f3e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T08:18:41.253342Z",
     "iopub.status.busy": "2024-04-05T08:18:41.253083Z",
     "iopub.status.idle": "2024-04-05T08:18:41.270682Z",
     "shell.execute_reply": "2024-04-05T08:18:41.269776Z"
    },
    "papermill": {
     "duration": 0.024208,
     "end_time": "2024-04-05T08:18:41.272729",
     "exception": false,
     "start_time": "2024-04-05T08:18:41.248521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_mean_std(features):\n",
    "    batch_size, c = features.size()[:2]\n",
    "    features_mean = features.reshape(batch_size, c, -1).mean(dim=2).reshape(batch_size, c, 1, 1)\n",
    "    features_std = features.reshape(batch_size, c, -1).std(dim=2).reshape(batch_size, c, 1, 1) + 1e-6\n",
    "    return features_mean, features_std\n",
    "\n",
    "\n",
    "def adain(content_features, style_features):\n",
    "    content_mean, content_std = calc_mean_std(content_features)\n",
    "    style_mean, style_std = calc_mean_std(style_features)\n",
    "    normalized_features = style_std * (content_features - content_mean) / content_std + style_mean\n",
    "    return normalized_features\n",
    "\n",
    "\n",
    "class NonLocalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
    "        super(NonLocalBlock, self).__init__()\n",
    "\n",
    "        self.sub_sample = sub_sample\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.inter_channels = inter_channels\n",
    "\n",
    "        if self.inter_channels is None:\n",
    "            self.inter_channels = in_channels // 2\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1\n",
    "\n",
    "        conv_nd = nn.Conv2d\n",
    "        max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        bn = nn.GroupNorm\n",
    "\n",
    "        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                         kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        if bn_layer:\n",
    "            self.W = nn.Sequential(\n",
    "                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                        kernel_size=1, stride=1, padding=0),\n",
    "                bn(16, self.in_channels)\n",
    "            )\n",
    "            nn.init.constant_(self.W[1].weight, 0)\n",
    "            nn.init.constant_(self.W[1].bias, 0)\n",
    "        else:\n",
    "            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "            nn.init.constant_(self.W.weight, 0)\n",
    "            nn.init.constant_(self.W.bias, 0)\n",
    "\n",
    "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                           kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        if sub_sample:\n",
    "            self.g = nn.Sequential(self.g, max_pool_layer)\n",
    "            self.phi = nn.Sequential(self.phi, max_pool_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
    "        g_x = g_x.permute(0, 2, 1)\n",
    "\n",
    "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
    "        theta_x = theta_x.permute(0, 2, 1)\n",
    "        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
    "        f = torch.matmul(theta_x, phi_x)\n",
    "        f_div_C = F.softmax(f, dim=-1)\n",
    "\n",
    "        y = torch.matmul(f_div_C, g_x)\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
    "        W_y = self.W(y)\n",
    "        z = W_y + x\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ab3b73",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-05T08:18:41.282569Z",
     "iopub.status.busy": "2024-04-05T08:18:41.282287Z",
     "iopub.status.idle": "2024-04-05T08:18:41.584996Z",
     "shell.execute_reply": "2024-04-05T08:18:41.584096Z"
    },
    "papermill": {
     "duration": 0.309862,
     "end_time": "2024-04-05T08:18:41.587076",
     "exception": false,
     "start_time": "2024-04-05T08:18:41.277214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50737/50737 [00:00<00:00, 328827.65it/s]\n",
      "100%|██████████| 3200/3200 [00:00<00:00, 314260.99it/s]\n"
     ]
    }
   ],
   "source": [
    "class VGG(nn.Module):   # output relu4-1\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        model = vgg19()\n",
    "        model.load_state_dict(torch.load('/kaggle/input/baid/pytorch/fully_pretrained/2/vgg19-dcbb9e9d.pth'),\n",
    "                                        strict=False)\n",
    "        self.model = nn.Sequential(*model.features[:21])\n",
    "        self._freeze_params()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def _freeze_params(self):\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, identity=False):\n",
    "        super(SAB, self).__init__()\n",
    "        model = resnet50()\n",
    "        self.model = nn.Module()\n",
    "        self.model.conv1 = model.conv1\n",
    "        self.model.bn1 = model.bn1\n",
    "        self.model.relu = model.relu\n",
    "        self.model.maxpool = model.maxpool\n",
    "\n",
    "        self.model.layer1 = model.layer1\n",
    "        self.model.layer2 = model.layer2\n",
    "\n",
    "        self.identity = identity\n",
    "\n",
    "        self.vgg = VGG()\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # aligned to the output of VGG\n",
    "        sty = self.vgg(x)\n",
    "        aes = self.model.conv1(x)\n",
    "        aes = self.model.bn1(aes)\n",
    "        aes = self.model.relu(aes)\n",
    "        aes = self.model.maxpool(aes)\n",
    "\n",
    "        aes = self.model.layer1(aes)\n",
    "        aes = self.model.layer2(aes)\n",
    "\n",
    "        output = adain(aes, sty)\n",
    "        if self.identity:\n",
    "            output += aes\n",
    "\n",
    "        return F.relu(output)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.load_state_dict(torch.load('/kaggle/input/baid/pytorch/fully_pretrained/2/epoch_99.pth'),\n",
    "                                        strict=False)\n",
    "        else:\n",
    "            self.model.load_state_dict(torch.load('/kaggle/input/baid/pytorch/fully_pretrained/2/epoch_99.pth', map_location=torch.device('cpu')),\n",
    "                                        strict=False)\n",
    "\n",
    "\n",
    "class GAB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAB, self).__init__()\n",
    "        model = resnet50()\n",
    "        self.model = nn.Module()\n",
    "        self.model.conv1 = model.conv1\n",
    "        self.model.bn1 = model.bn1\n",
    "        self.model.relu = model.relu\n",
    "        self.model.maxpool = model.maxpool\n",
    "\n",
    "        self.model.layer1 = model.layer1\n",
    "        self.model.layer2 = model.layer2\n",
    "        self.model.layer3 = model.layer3\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _init_weights(self):\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.load_state_dict(torch.load('/kaggle/input/baid/pytorch/fully_pretrained/2/epoch_99.pth'),\n",
    "                                        strict=False)\n",
    "        else:\n",
    "            self.model.load_state_dict(torch.load('/kaggle/input/baid/pytorch/fully_pretrained/2/epoch_99.pth', map_location=torch.device('cpu')),\n",
    "                                        strict=False)\n",
    "class SAAN(nn.Module):\n",
    "    def __init__(self, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.GenAes = GAB()\n",
    "        self.StyAes = SAB()\n",
    "\n",
    "        self.NLB = NonLocalBlock(in_channels=1536)\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=(2, 2))\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(num_features=1536)\n",
    "\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(1536 * 4, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(2048, num_classes),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self._initial_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gen_aes = self.GenAes(x)\n",
    "        sty_aes = self.StyAes(x)\n",
    "\n",
    "        sty_aes = self.max_pool(sty_aes)\n",
    "\n",
    "        all_aes = torch.cat((sty_aes, gen_aes), 1)\n",
    "        all_aes = self.NLB(all_aes)\n",
    "\n",
    "        all_aes = self.avg_pool(all_aes)\n",
    "        all_aes = self.bn(all_aes)\n",
    "\n",
    "        fc_input = torch.flatten(all_aes, start_dim=1)\n",
    "\n",
    "        output = self.predictor(fc_input)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _initial_weights(self):\n",
    "        for m in self.bn.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.normal_(m.weight.data, mean=1.0, std=0.02)\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "        for m in self.predictor.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=0.02)\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "class ResNetPretrain(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetPretrain, self).__init__()\n",
    "        model = resnet50()\n",
    "        self.model = model\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=2048, out_features=num_classes, bias=True),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        self._initial_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        features = self.model.layer4(x)\n",
    "        features_flat = self.model.avgpool(features)\n",
    "        features_flat = torch.flatten(features_flat, 1)\n",
    "        output = self.model.fc(features_flat)\n",
    "\n",
    "        return features, output\n",
    "\n",
    "    def _initial_weights(self):\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "n_epochs = 44\n",
    "batch_size = 64\n",
    "validation_frequency = 2\n",
    "save_frequency = 2\n",
    "learning_rate = 1e-5\n",
    "device = \"cuda\"\n",
    "checkpoint_dir = \"/kaggle/working/SAAN\"\n",
    "\n",
    "train_dataset = BBDataset(file_dir='/kaggle/input/baid-csvs', type='train', test=False)\n",
    "val_dataset = BBDataset(file_dir='/kaggle/input/baid-csvs', type='validation', test=True)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(learning_rate, optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 10 epochs\"\"\"\n",
    "    if epoch < 40:\n",
    "        lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "    else:\n",
    "        lr = learning_rate * (0.1 ** 4)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def save_checkpoint(checkpoint_dir, model, epoch):\n",
    "    checkpoint_dir = checkpoint_dir\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    save_path = os.path.join(checkpoint_dir, f\"epoch_{epoch}.pth\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "def validate(model, val_loader, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for step, val_data in enumerate(val_loader):\n",
    "            image = val_data[0].to(device)\n",
    "            label = val_data[1].to(device).float()\n",
    "\n",
    "            predicted_label = model(image).squeeze()\n",
    "            val_loss += loss(predicted_label, label).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(\"Epoch: %3d Validation loss: %.8f\" % (epoch, val_loss))\n",
    "\n",
    "\n",
    "def train():\n",
    "    loss_hist = []\n",
    "    model = SAAN(num_classes=1)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'GenAes' in name:\n",
    "            param.requires_grad = False\n",
    "    model = model.to(device)\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999), weight_decay=5e-4)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=8)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for step, train_data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            image = train_data[0].to(device)\n",
    "            label = train_data[1].to(device).float()\n",
    "\n",
    "            predicted_label = model(image).squeeze()\n",
    "            train_loss = loss(predicted_label, label)\n",
    "\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += train_loss.item()\n",
    "\n",
    "        print(\"Epoch: %3d Step: %5d / %5d Train loss: %.8f\" % (epoch, step, len(train_loader), train_loss.item()))\n",
    "        loss_hist.append(epoch_loss)\n",
    "        adjust_learning_rate(learning_rate, optimizer, epoch)\n",
    "\n",
    "        if (epoch + 1) % validation_frequency == 0:\n",
    "            validate(model, val_loader, epoch)\n",
    "            print(\"Validation\")\n",
    "\n",
    "        if (epoch + 1) % save_frequency == 0:\n",
    "            save_checkpoint(checkpoint_dir, model, epoch)\n",
    "            print(\"Saved\")\n",
    "        \n",
    "\n",
    "    print(\"Done training\")\n",
    "    return loss_hst\n",
    "\n",
    "\n",
    "\n",
    "# Uncomment when you want to train\n",
    "# Might take up to a day to train on two GPU T4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65c9fd04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T08:18:41.596976Z",
     "iopub.status.busy": "2024-04-05T08:18:41.596671Z",
     "iopub.status.idle": "2024-04-05T19:25:17.533037Z",
     "shell.execute_reply": "2024-04-05T19:25:17.531469Z"
    },
    "papermill": {
     "duration": 39995.947799,
     "end_time": "2024-04-05T19:25:17.539407",
     "exception": true,
     "start_time": "2024-04-05T08:18:41.591608",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 Step:   792 /   793 Train loss: 0.06662835\n",
      "Epoch:   1 Step:   792 /   793 Train loss: 0.03847170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 Validation loss: 0.01915823\n",
      "Validation\n",
      "Saved\n",
      "Epoch:   2 Step:   792 /   793 Train loss: 0.05486448\n",
      "Epoch:   3 Step:   792 /   793 Train loss: 0.03501185\n",
      "Epoch:   3 Validation loss: 0.01868332\n",
      "Validation\n",
      "Saved\n",
      "Epoch:   4 Step:   792 /   793 Train loss: 0.02957827\n",
      "Epoch:   5 Step:   792 /   793 Train loss: 0.03639141\n",
      "Epoch:   5 Validation loss: 0.01606507\n",
      "Validation\n",
      "Saved\n",
      "Epoch:   6 Step:   792 /   793 Train loss: 0.02503256\n",
      "Epoch:   7 Step:   792 /   793 Train loss: 0.02657691\n",
      "Epoch:   7 Validation loss: 0.01514183\n",
      "Validation\n",
      "Saved\n",
      "Epoch:   8 Step:   792 /   793 Train loss: 0.03429513\n",
      "Epoch:   9 Step:   792 /   793 Train loss: 0.02019162\n",
      "Epoch:   9 Validation loss: 0.01504720\n",
      "Validation\n",
      "Saved\n",
      "Epoch:  10 Step:   792 /   793 Train loss: 0.02707661\n",
      "Epoch:  11 Step:   792 /   793 Train loss: 0.02042348\n",
      "Epoch:  11 Validation loss: 0.01463178\n",
      "Validation\n",
      "Saved\n",
      "Epoch:  12 Step:   792 /   793 Train loss: 0.01590515\n",
      "Epoch:  13 Step:   792 /   793 Train loss: 0.03700113\n",
      "Epoch:  13 Validation loss: 0.01483027\n",
      "Validation\n",
      "Saved\n",
      "Epoch:  14 Step:   792 /   793 Train loss: 0.04094507\n",
      "Epoch:  15 Step:   792 /   793 Train loss: 0.03100188\n",
      "Epoch:  15 Validation loss: 0.01446564\n",
      "Validation\n",
      "Saved\n",
      "Epoch:  16 Step:   792 /   793 Train loss: 0.02457080\n",
      "Epoch:  17 Step:   792 /   793 Train loss: 0.01808816\n",
      "Epoch:  17 Validation loss: 0.01458794\n",
      "Validation\n",
      "Saved\n",
      "Epoch:  18 Step:   792 /   793 Train loss: 0.02991134\n",
      "Epoch:  19 Step:   792 /   793 Train loss: 0.01477736\n",
      "Epoch:  19 Validation loss: 0.01450041\n",
      "Validation\n",
      "Saved\n",
      "Epoch:  20 Step:   792 /   793 Train loss: 0.01744039\n",
      "Epoch:  21 Step:   792 /   793 Train loss: 0.04048606\n",
      "Epoch:  21 Validation loss: 0.01433434\n",
      "Validation\n",
      "Saved\n",
      "Epoch:  22 Step:   792 /   793 Train loss: 0.02352647\n",
      "Epoch:  23 Step:   792 /   793 Train loss: 0.02366908\n",
      "Epoch:  23 Validation loss: 0.01447295\n",
      "Validation\n",
      "Saved\n",
      "Epoch:  24 Step:   792 /   793 Train loss: 0.02426924\n",
      "Epoch:  25 Step:   792 /   793 Train loss: 0.03479211\n",
      "Epoch:  25 Validation loss: 0.01435847\n",
      "Validation\n",
      "Saved\n",
      "Epoch:  26 Step:   792 /   793 Train loss: 0.03670301\n",
      "Epoch:  27 Step:   792 /   793 Train loss: 0.03063639\n",
      "Epoch:  27 Validation loss: 0.01462917\n",
      "Validation\n",
      "Saved\n",
      "Epoch:  28 Step:   792 /   793 Train loss: 0.02005789\n",
      "Epoch:  29 Step:   792 /   793 Train loss: 0.01813938\n",
      "Epoch:  29 Validation loss: 0.01444269\n",
      "Validation\n",
      "Saved\n",
      "Epoch:  30 Step:   792 /   793 Train loss: 0.01788486\n",
      "Epoch:  31 Step:   792 /   793 Train loss: 0.03138266\n",
      "Epoch:  31 Validation loss: 0.01447888\n",
      "Validation\n",
      "Saved\n",
      "Epoch:  32 Step:   792 /   793 Train loss: 0.02283994\n",
      "Epoch:  33 Step:   792 /   793 Train loss: 0.03730223\n",
      "Epoch:  33 Validation loss: 0.01455869\n",
      "Validation\n",
      "Saved\n",
      "Epoch:  34 Step:   792 /   793 Train loss: 0.04977581\n",
      "Epoch:  35 Step:   792 /   793 Train loss: 0.02155667\n",
      "Epoch:  35 Validation loss: 0.01451329\n",
      "Validation\n",
      "Saved\n",
      "Epoch:  36 Step:   792 /   793 Train loss: 0.02081790\n",
      "Epoch:  37 Step:   792 /   793 Train loss: 0.01661369\n",
      "Epoch:  37 Validation loss: 0.01474112\n",
      "Validation\n",
      "Saved\n",
      "Epoch:  38 Step:   792 /   793 Train loss: 0.03378714\n",
      "Epoch:  39 Step:   792 /   793 Train loss: 0.02270332\n",
      "Epoch:  39 Validation loss: 0.01444859\n",
      "Validation\n",
      "Saved\n",
      "Epoch:  40 Step:   792 /   793 Train loss: 0.02663748\n",
      "Epoch:  41 Step:   792 /   793 Train loss: 0.01897720\n",
      "Epoch:  41 Validation loss: 0.01440936\n",
      "Validation\n",
      "Saved\n",
      "Epoch:  42 Step:   792 /   793 Train loss: 0.02233888\n",
      "Epoch:  43 Step:   792 /   793 Train loss: 0.02392008\n",
      "Epoch:  43 Validation loss: 0.01456290\n",
      "Validation\n",
      "Saved\n",
      "Done training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss_hst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss_hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 289\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss_hst\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_hst' is not defined"
     ]
    }
   ],
   "source": [
    "loss_hist = train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ee9d7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T18:06:16.323103Z",
     "iopub.status.busy": "2024-04-04T18:06:16.322819Z",
     "iopub.status.idle": "2024-04-04T18:06:16.664938Z",
     "shell.execute_reply": "2024-04-04T18:06:16.663740Z",
     "shell.execute_reply.started": "2024-04-04T18:06:16.323077Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Loss plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f4d174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T06:51:19.785393Z",
     "iopub.status.busy": "2024-04-05T06:51:19.784709Z",
     "iopub.status.idle": "2024-04-05T06:53:36.357297Z",
     "shell.execute_reply": "2024-04-05T06:53:36.356291Z",
     "shell.execute_reply.started": "2024-04-05T06:51:19.785359Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = BBDataset(file_dir='/kaggle/input/baid-csvs', type='test', test=True)\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    device = \"cuda\"\n",
    "    checkpoint_path = \"/kaggle/input/baid_variant/pytorch/grouponly/1/epoch_1.pth\"\n",
    "    df = pd.read_csv('/kaggle/input/baid-csvs/test_set.csv')\n",
    "    predictions = []\n",
    "\n",
    "    model = SAAN(num_classes=1)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    model.eval()\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True, num_workers=8)\n",
    "    print(\"test\")\n",
    "    with torch.no_grad():\n",
    "        for step, test_data in tqdm(enumerate(test_loader)):\n",
    "            image = test_data[0].to(device)\n",
    "\n",
    "            predicted_label = model(image)\n",
    "            prediction = predicted_label.squeeze().cpu().numpy()\n",
    "            predictions.append(prediction * 10)\n",
    "\n",
    "    scores = df['score'].values.tolist()\n",
    "\n",
    "    print(scipy.stats.spearmanr(scores, predictions))\n",
    "    print(scipy.stats.pearsonr(scores, predictions))\n",
    "\n",
    "    acc = 0\n",
    "    for i in range(len(scores)):\n",
    "        cls1 = 1 if scores[i] > 5 else 0\n",
    "        cls2 = 1 if predictions[i] > 5 else 0\n",
    "        if cls1 == cls2:\n",
    "            acc += 1\n",
    "    print(acc/len(scores))\n",
    "    df.insert(loc=2, column='prediction', value=predictions)\n",
    "    \n",
    "    save_dir = '/kaggle/working'\n",
    "\n",
    "    save_path = os.path.join(save_dir, 'result.csv')\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(\"done\")\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b75d3e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4642477,
     "sourceId": 7908673,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4645951,
     "sourceId": 7908740,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 15809,
     "sourceId": 19078,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 16205,
     "sourceId": 19540,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 21755,
     "sourceId": 25842,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 40012.283965,
   "end_time": "2024-04-05T19:25:20.603947",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-05T08:18:28.319982",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
